\section{Machine Learning}
\label{sec:ml}


\subsection{Dynamic Time Warping - INGLOBARE NEL CAPITOLO MACHINE LEARNING}
\textbf{Dynamic Time Warping} or \textbf{DTW}, is an algorithm used to measure similarity between two time series even if they are of different lengths and/or have repetitions or deletions in them. If we view every single flow $F$ as a time series of packets $p_i,\dots p_m$, we can use \textbf{DTW} to measure how similar two flows are. The reason we are interested in this will become clear later.

\subsection{Machine Learning}
Ideal goal: we want to develop a machine learning algorithm that outputs the \textit{action label} $a_i$ given the \textit{action's flows} $A_i = [F_1,\dots, F_n]$.

Given the fact that each action generates multiple flows of different lengths, we know that standard supervised learning approaches are hard to apply. To see why standard approaches would not work we need to think about the structure of the input and output spaces. Our output space i.e. what we want to predict would be the \textit{action label} $a_i$, while our input space, i.e. the predictors, would be the flows generated by $a_i$ which we denote as $[F_1,\dots,F_n]_i $. One way we could represent flows as features would be to have a feature for each flow $F_j$ generated by the action $a_i$, and the value of a feature would be the sequence of byte sizes of the flow $[p_1,\dots, p_m]_j$. Because of the different number of flows of each action ($n$) we would immediately see that each row could possibly have a different number of features. The main problem of this approach is that we are artificially defining features with no real justification; in other words, we have no reason to associate the first flow of an action with the first flow of another action by marking them as the first feature of the respective samples.

To overcome this obstacle the original authors applied a two stage process. In the first stage, by using an unsupervised clustering algorithm they addressed their missing knowledge about the number of flows and their features. The aim of the first stage is to identify some features for our actions. The second stage exploits the just found features to perform a the canonic classification.
\subsubsection{Hierachical Agglomerative Clustering}
In the typical clustering scenario the goal of the algorithm is to find $k$ groups called clusters, where the \textit{inter-cluster} similarity is very high while the \textit{intra-cluster} similarity is minimal, meaning that the samples belonging to one clusters are very similar to each other while still being very different from samples of other clusters. Notice that the number of clusters $k$ and the similarity function between samples $f\_dist()$ have to be explicitly defined by the teacher. The output of a clustering algorith \todo{finire spiegazione}

In my specific implementation $k = 50$ and $f_dist = dtw$.
In my specific implementation I use as samples all the flows by themselves, ignoring temporarily the idea of action. The reason for this is that w
\subsubsection{Random Forest}
