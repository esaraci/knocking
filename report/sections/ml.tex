\section{Machine Learning}
\label{sec:ml}

\textbf{Goal}: as in Figure~\ref{fig:ideal} we want a machine learning algorithm $\mathcal{L}$ that predict the \textit{action label} $a_i$ given the \textit{action's flows} $A_i = [F_1,\dots, F_n]_i$.

\begin{figure}[h]
 \centering
 \includegraphics{images/ideal}
 \caption{\small{The ideal model predicti the label $a_i$ for the a new input instance $A_i$.}}
 \label{fig:ideal}
\end{figure}

Given the fact that each action generates multiple flows of different lengths, we know that standard supervised learning approaches are hard to apply. To see why standard approaches would not work we need to think about the structure of the input and output spaces. Our output space i.e. what we want to predict would be the \textit{action label} $a_i$, while our input space, i.e. the predictors, would be the flows generated by $a_i$ which we denote as $[F_1,\dots,F_n]_i $. One way we could represent flows as features would be to have a feature for each flow $F_j$ generated by the action $a_i$, and the value of the $j$-th feature would be the sequence of byte sizes of the flow $[p_1,\dots, p_m]_j$. Because of the different number of flows for each action  $n$ we would immediately see that each row could possibly have a different number of features. The main problem of this approach, other than the just mentioned diverse dimensionality, is that we are artificially defining features with no real justification; in other words, we have no reason to associate the first flow of an action $A_i$ with the first flow of another action $A_j$ by viewing them as the first feature of their respective samples.


To overcome this obstacle the original authors applied a two stage process. In the first stage, by using an unsupervised clustering algorithm, they addressed their missing knowledge about the number of flows and their features. The objective is to identify some kind of intermediate representation of the data $\Phi(A_i)$ that can be exploited by the second stage of the process, which will perform a canonic classification (Figure~\ref{fig:actual}). 
In the remainder of this section we describe the algorithms used in the two stage process and my personal implementation.\todo{decidere come strutturare sta roba.}

\begin{figure}[h]
 \centering
 \includegraphics{images/actual}
 \caption{\small{The two step classification of a new sample; first the new sample $A_i$ goes through the clustering algorithm which computes the intermediate representation $\Phi(A_i)$, then $\Phi(A_i)$ is used by the classifier to predict the correct label $a_i$. $k$ is the number of clusters, a more detailed description of $k$ and $\Phi$ can be found in Section \todo{Sezione implementativa}}}
 \label{fig:actual}
\end{figure}

\subsection{Clustering}
In the typical unsupervised clustering scenario the goal of the algorithm is to find $k$ groups called clusters, where the \textit{inter-cluster} similarity is very low while the \textit{intra-cluster} similarity is maximized, meaning that samples belonging to one clusters are very similar to each other while still being very different from samples belonging to other clusters. Notice that the number of clusters $k$ and the similarity function between samples have to be explicitly defined by the teacher. The input of the clustering algorithm is usually the whole datset while its output is a list of intergers $[c_i,\dots, c_N]$ where $N$ is the total number of samples and $c_i \in [1;k]$ is a an integer number that denotes the cluster to which the $i$-th sample has been assigned by the algorithm.
\subsubsection{Hierarchical Agglomerative Clustering}
In this specific instance I used (as suggeseted in the papers) a \textbf{Hierachical Agglomerative Clustering}, \textbf{HAC} for short. HAC starts by creating a cluster for each flow; at each step HAC reduces the number of clusters by merging (agglomerative) the closest clusters together based on the distance function and other parameters; HAC will stop whenever the desired number of clusters $k$ is reached. The structure that HAC generates can be viewed as a tree (hierachical), that structure is called dendrogram; in Figure \ref{fig:dendro} we can see the dendrogram for 20 samples (i.e. 20 flows). In my implementation the input consists in all the flows contained in the dataset\footnote{We are just using flows now, the concept ``actions'' is being ignored for the moment since it is not really needed for the clustering stage.}, the number of clusters $k$ is set to $50$, and the distance function used to compute similarity between flows is the Dynamic Time Warping described in the following paragraph.

\begin{figure}[!h]
 \centering
 \includegraphics[scale=0.5]{images/dendrogram}
 \caption{\small{Dendrogram for 20 flows; the number of clusters can be seen by counting how many vertical lines get crossed by an imaginary horizontal line. Different horizontal lines will of course determine different clusters and different distance measures; the dendrogram allows the designer to decide at what level to draw the line i.e. which value to assign to $k$ by looking at the distance value and at the hierarchy itself.}}
 \label{fig:dendro}
\end{figure}

\subsubsection{Dynamic Time Warping} \textbf{DTW} is an algorithm used generally to find the best match between two time series even when they have different lengths and/or have repetitions or deletions in them. If we view every single flow $F_i$ as a series of packets $[p_1,\dots p_m]_i$, we can use \textbf{DTW} to measure how similar two flows are. This similarity measure is needed by the HAC algorithm to compute the distances between clusters.

\subsubsection{Feature Extraction}

The HAC output is, as stated before, a list of $N$ integers $[c_1,\dots, c_N]$ where $c_i$ is the cluster assigned to the $i$-th input sample. This is different from what is reported in Figure \ref{fig:ideal}, in fact the clustering result needs to be rearranged before being given as input to the classifier. The aim of the rearrangement is to actually generate a new intermediate dataset where the target we want to predict are still the actions' labels but the features are now different. We generate $k$ features for each action $A_i$; the value of the $j$-th feature is the number of flows of $A_i$ that ended in the $j$-th cluster during the clustering algorithm. This whole process composed by the \textit{clustering} and the final rearrangement is what we call feature extraction and denote with the symbol $\Phi$, so $\Phi(A_i)$ is the intermediate representation of $A_i$ and is considered to be a single input sample of the new intermediate dataset.

\begin{table}[]
\label{tab:newdataset}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{action\_label}     & \multicolumn{1}{l}{\textbf{C1}} & \multicolumn{1}{l}{\textbf{C2}} & \multicolumn{1}{l}{\textbf{\dots}} & \multicolumn{1}{l}{\textbf{C50}} \\ \midrule
open facebook              & 0                               & 11                              & \dots                              & 6                                \\
writing search             & 1                               & 1                               & \dots                              & 5                                \\
back to news               & 0                               & 0                               & \dots                              & 0                                \\
open facebook              & 0                               & 9                               & \dots                              & 5                                \\
\multicolumn{1}{c}{\vdots} & \vdots                          &                                 & \vdots                             & \vdots                           \\ \bottomrule
\end{tabular}
\caption{\small{Format of the new dataset. We can see that 11 flows of the first action ended up in cluster number 2, while 6 of them ended up in the last cluster.}}
\end{table}

\subsubsection{Classification and Random Forest}
Random forest is a very good algorithm
