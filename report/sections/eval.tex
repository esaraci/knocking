\section{Evaluation}
\label{sec:eval}
\subsection{Experimental Setup}
For this work I implemented two scripts using Python 3.6.5. The two scripts represent the two stages of clustering and classification shown in Figure~\ref{fig:actual}; the first script, named \texttt{clustering.py}, performs the clustering and saves the intermediate representation as a .csv file called \texttt{[appname]\_dataset.csv}. This file is then read as input by \texttt{classifier.py} which will consequently perform the classification using the Random Forest algorithm presented in Section~\ref{subsubsec:rfc}. The computed performance measures will be printed to \texttt{stdout}.

It has to be noted that I trained a model for each different application, this means that the model that classifies Facebook actions has no way to classify Tumblr actions since none of the Tumblr flows has been shown to it during the clustering process or during the Random Forest training phase.

Most of the actions collected in the initial dataset are deemed to be not privacy-violating by the original authors; I do share their opinion, for this reason, before training the Random Forest, I renamed all the labels of the non relevant actions as \textit{other}. This operation is done in memory by the \texttt{classifier.py} script, in this way the intermediate dataset stays untouched and many different configuration of relevant actions can be tried. The list of relevant action for each app can be found inside \texttt{classification.py}.


\subsubsection{Computational Issue}
Because of some issues with the functions provided by the \textit{SciPy} library, I had to keep in memory the distance matrix of the flows. In the distance matrix $D$ each entry $(i,j)$ contains the value of $dtw(i, j)$ which makes $D$ symmetric, which reduces by half the memory needed for the matrix since I could just compute the triangular upper (or lower) part of $D$. Unfortunately this ``optimization'' was not enough; the amount of memory needed was way higher that what my Google Cloud machine had. For this reason I decided to reduce the dataset to about 10,000 flows per application; again this is done in memory by the \textit{clustering.py} script, therefore the original dataset is not modified. After the clustering phase, the generated intermediate dataset contains a number of samples (actions) varying from 2000 to 5000 based on which application is being analyzed. One intermediate dataset is generated for each application.

\todo{qui cito i praametri/input/output di entrambi gli algoritmi}
\subsection{Experimental Results}
\todo{le confusion matrix, precision recall AP}

% ['dropbox', 'evernote', 'facebook', 'gmail', 'gplus', 'tumblr',
%        'twitter'], dtype=object), array([48462, 17014, 50319,  9924, 33471, 56702, 36259]))

% actions in original dataset
% Actions for dropbox: 15103
% Actions for evernote: 6214
% Actions for facebook: 12468
% Actions for gmail: 5644
% Actions for gplus: 17573
% Actions for tumblr: 14989
% Actions for twitter: 7334

% Actions for dropbox: 33
% Actions for evernote: 32
% Actions for facebook: 41
% Actions for gmail: 38
% Actions for gplus: 38
% Actions for tumblr: 47
% Actions for twitter: 25
